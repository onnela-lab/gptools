{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference of coin bias\n",
    "\n",
    "This example illustrates variational Bayesian inference of the bias of a coin. The model is\n",
    "$$\\begin{align}\n",
    "p&\\sim\\text{Beta}\\left(10, 10\\right)\\\\\n",
    "x&\\sim\\text{Bernoulli}\\left(p\\right),\n",
    "\\end{align}$$\n",
    "where $p$ is the bias of the coin, and $x$ are Bernoulli trials. We use Monte Carlo estimates of the evidence lower bound to learn the parameters of the variational approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptools.torch.util import ParametrizedDistribution, VariationalModel\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoinModel(VariationalModel):\n",
    "    \"\"\"\n",
    "    Simple model for inferring the bias of a coin (see \n",
    "    https://pyro.ai/examples/svi_part_i.html#A-simple-example for details).\n",
    "    \"\"\"\n",
    "    def __init__(self, approximations, x) -> None:\n",
    "        super().__init__(approximations)\n",
    "        self.x = x\n",
    "        self.prior = th.distributions.Beta(10, 10)\n",
    "\n",
    "    def log_prob(self, parameters: dict[str, th.Tensor]) -> th.Tensor:\n",
    "        # Evaluate the probability of the bias `p` under the prior and the likelihood.\n",
    "        return self.prior.log_prob(parameters[\"p\"]) \\\n",
    "            + th.distributions.Bernoulli(parameters[\"p\"][..., None]).log_prob(self.x).sum(axis=-1)\n",
    "\n",
    "\n",
    "# Generate some data and initialize the model with an uninformative posterior. Instatiating with\n",
    "# the prior could also be an option.\n",
    "x = (th.arange(10) < 6).to(float)\n",
    "model = CoinModel({\n",
    "    \"p\": ParametrizedDistribution(th.distributions.Beta, concentration0=1, concentration1=1),\n",
    "}, x)\n",
    "model.check_log_prob_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the variational approximation.\n",
    "batch_size = 100\n",
    "optim = th.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "losses = []\n",
    "for _ in range(1 if \"CI\" in os.environ else 2000):\n",
    "    optim.zero_grad()\n",
    "    loss = - model.batch_elbo_estimate((batch_size,)).mean()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss and posterior.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(losses)\n",
    "ax1.set_ylabel(\"Negative ELBO\")\n",
    "ax1.set_xlabel(\"Iteration\")\n",
    "\n",
    "lin = th.linspace(0, 1, 100)\n",
    "ax2.plot(lin, model.prior.log_prob(lin).exp(), label=\"prior\")\n",
    "ax2.plot(lin, model.distributions()[\"p\"].log_prob(lin).exp().detach(), label=\"approx. posterior\")\n",
    "posterior = th.distributions.Beta(model.prior.concentration0 + (x == 1).sum(),\n",
    "                                  model.prior.concentration1 + (x == 0).sum())\n",
    "ax2.plot(lin, posterior.log_prob(lin).exp(), label=\"exact posterior\")\n",
    "ax2.axvline(x.mean(), color=\"k\", ls=\"--\", label=\"MLE\")\n",
    "ax2.set_xlabel(\"Coin bias $p$\")\n",
    "ax2.set_ylabel(\"Posterior density\")\n",
    "ax2.set_ylim(top=6.5)\n",
    "ax2.legend(fontsize=\"small\")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce292631760a50db98487f107c57a4e83e7303c2c65ea0b6fb35e4138a49650e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
