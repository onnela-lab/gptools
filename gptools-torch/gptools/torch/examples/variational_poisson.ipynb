{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptools.util.kernels import ExpQuadKernel\n",
    "from gptools.torch import GraphGaussianProcess, ParametrizedDistribution, \\\n",
    "    TerminateOnPlateau, VariationalModel\n",
    "from gptools.util import coordgrid\n",
    "from gptools.util.graph import lattice_predecessors, LatticeBounds, num_lattice_predecessors\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "import torch as th\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we consider a two-dimensional Poisson count model with latent log rate that follows a Gaussian process. Inference proceeds by minimizing a Monte Carlo estimate of the evidence lower bound under a factorized posterior approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameter values and sample from the Gaussian process.\n",
    "width = 80\n",
    "height = 90\n",
    "kernel = ExpQuadKernel(1.1, 4, 1e-3)\n",
    "k = 5\n",
    "mu = -1\n",
    "seed = 0\n",
    "bounds = LatticeBounds.ELLIPSE\n",
    "\n",
    "x = th.arange(width)\n",
    "y = th.arange(height)\n",
    "coords = th.as_tensor(coordgrid(x, y))\n",
    "\n",
    "shape = (width, height)\n",
    "predecessors = lattice_predecessors(shape, k, bounds=bounds)\n",
    "dist = GraphGaussianProcess(mu * th.ones(width * height), coords, predecessors, kernel)\n",
    "\n",
    "th.manual_seed(seed)\n",
    "eta = dist.sample().reshape(shape)\n",
    "lam = eta.exp()\n",
    "counts = th.distributions.Poisson(lam).sample()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "vmin = min(lam.min(), counts.min())\n",
    "vmax = max(lam.max(), counts.max())\n",
    "ax1.pcolormesh(x, y, lam.T, vmin=vmin, vmax=vmax)\n",
    "ax2.pcolormesh(x, y, counts.T, vmin=vmin, vmax=vmax)\n",
    "ax1.set_title(r\"Density $\\lambda=\\exp\\eta$\")\n",
    "ax2.set_title(r\"Counts $y \\sim\\mathrm{Poisson}\\left(\\lambda\\right)$\")\n",
    "ax1.set_aspect(\"equal\")\n",
    "ax2.set_aspect(\"equal\")\n",
    "fig.tight_layout()\n",
    "\n",
    "# Ravel the counts for the rest of the notebook.\n",
    "counts = counts.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a variational model.\n",
    "class SpatialCountModel(VariationalModel):\n",
    "    def __init__(self, approximations, counts):\n",
    "        super().__init__(approximations)\n",
    "        self.counts = counts\n",
    "        \n",
    "    def log_prob(self, parameters):\n",
    "        lam = parameters[\"eta\"].exp()\n",
    "        mu = parameters[\"mu\"]\n",
    "        dist = GraphGaussianProcess(mu[..., None] * th.ones_like(self.counts), coords, predecessors, kernel)\n",
    "        return dist.log_prob(parameters[\"eta\"]) \\\n",
    "            + th.distributions.Poisson(lam).log_prob(self.counts).sum(axis=-1)\n",
    "     \n",
    "\n",
    "# Initialize using Laplace approximation with flat prior.\n",
    "eta_loc = (counts + 0.1).log()\n",
    "eta_scale = (- eta_loc / 2).exp()\n",
    "model = SpatialCountModel({\n",
    "    \"eta\": ParametrizedDistribution(th.distributions.Normal, loc=eta_loc, scale=eta_scale),\n",
    "    \"mu\": ParametrizedDistribution(th.distributions.Normal, loc=0.0, scale=1.0),\n",
    "}, counts)\n",
    "model.check_log_prob_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model ...\n",
    "batch_size = 10\n",
    "optim = th.optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=20, verbose=True)\n",
    "\n",
    "losses = []\n",
    "terminator = TerminateOnPlateau(30, max_num_steps=float(os.environ.get(\"MAX_NUM_STEPS\", \"inf\")))\n",
    "with tqdm() as progress:\n",
    "    while terminator:\n",
    "        optim.zero_grad()\n",
    "        loss = - model.batch_elbo_estimate((batch_size,)).mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "        scheduler.step(loss)\n",
    "        terminator.step(loss)\n",
    "        progress.update()\n",
    "        progress.set_description(\n",
    "            f\"loss: {loss.item():.3f}; \"\n",
    "            f\"termination plateau: {terminator.elapsed} / {terminator.patience}\"\n",
    "        )\n",
    "    \n",
    "\n",
    "# ... and show the losses from the last batch.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the true density with the inferred density.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "posterior = model.distributions()[\"eta\"]\n",
    "estimate = posterior.mean.detach().reshape(shape).T.exp()\n",
    "vmin = min(lam.min(), estimate.min())\n",
    "vmax = max(lam.max(), estimate.max())\n",
    "\n",
    "ax1.pcolormesh(x, y, lam.T, vmin=vmin, vmax=vmax)\n",
    "ax2.pcolormesh(x, y, estimate, vmin=vmin, vmax=vmax)\n",
    "ax1.set_title(r\"Density $\\lambda=\\exp\\eta$\")\n",
    "ax2.set_title(r\"Inferred density\")\n",
    "ax1.set_aspect(\"equal\")\n",
    "ax2.set_aspect(\"equal\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the inferred parameters.\n",
    "fig, ax = plt.subplots()\n",
    "dist = model.distributions()[\"mu\"]\n",
    "with th.no_grad():\n",
    "    lin = dist.mean + 3 * dist.scale * th.linspace(-1, 1, 100)\n",
    "    ax.plot(lin, dist.log_prob(lin).exp())\n",
    "ax.axvline(mu, color=\"k\", ls=\"--\")\n",
    "ax.set_xlabel(r\"Mean $\\mu$\")\n",
    "ax.set_ylabel(\"Posterior density\")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce292631760a50db98487f107c57a4e83e7303c2c65ea0b6fb35e4138a49650e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
