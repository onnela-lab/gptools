{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian process Poisson regression\n",
    "\n",
    "Poisson regression is suitable for estimating the latent rate of count data, such as the number of buses arriving at a stop within an hour or the number of car accidents per day. Here, we estimate the rate using a Gaussian process given synthetic count data. First, we draw the log rate $\\eta$ from a Gaussian process with squared exponential kernel $k$ observed at discrete positions $x$, i.e. the covariance of two observations is\n",
    "$$\n",
    "\\mathrm{cov}\\left(\\eta(x),\\eta(y)\\right)=\\sigma^2\\exp\\left(-\\frac{d(x, y)^2}{2\\ell^2}\\right) + \\epsilon\\delta(x - y),\n",
    "$$\n",
    "where $d\\left(\\cdot,\\cdot\\right)$ is a distance measure, $\\sigma$ is the marginal scale of the process, and $\\ell$ is its correlation length. The \"nugget\" variance $\\epsilon$ ensures the covariance matrix is numerically positive semi-definite. Second, we sample counts $y$ from a Poisson distribution with rate $\\lambda = \\exp\\eta$. Let's generate a sample with $n$ regularly-spaced observations and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptools.util.kernels import DiagonalKernel, ExpQuadKernel, Kernel\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def simulate(x: np.ndarray, kernel: Kernel, mu: float = 0) -> dict:\n",
    "    \"\"\"Simulate a Gaussian process Poisson model with mean mu observed at x.\"\"\"\n",
    "    # Add an extra dimension to the vector x because the kernel expects an array\n",
    "    # with shape (num_data_points, num_dimensions).\n",
    "    X = x[:, None]\n",
    "    cov = kernel.evaluate(X)\n",
    "    eta = np.random.multivariate_normal(mu * np.ones_like(x), cov)\n",
    "    rate = np.exp(eta)\n",
    "    y = np.random.poisson(rate)\n",
    "    # Return the results as a dictionary, including input arguments (we need to extract kernel \n",
    "    # parameters from the `CompositeKernel`).\n",
    "    return {\"x\": x, \"X\": X, \"mu\": mu, \"y\": y, \"eta\": eta, \"y\": y, \"rate\": rate, \"n\": x.size,\n",
    "            \"sigma\": kernel.a.sigma, \"length_scale\": kernel.a.length_scale, \n",
    "            \"epsilon\": kernel.b.epsilon}\n",
    "\n",
    "\n",
    "def plot_sample(sample: dict, ax: mpl.axes.Axes = None) -> mpl.axes.Axes:\n",
    "    \"\"\"Visualize a sample of a Gaussian process Poisson model.\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    ax.plot(sample[\"x\"], sample[\"rate\"], label=r\"rate $\\lambda$\")\n",
    "    ax.scatter(sample[\"x\"], sample[\"y\"], marker=\".\", color=\"k\", label=\"counts $y$\",\n",
    "               alpha=0.5)\n",
    "    ax.set_xlabel(\"Location $x$\")\n",
    "    return ax\n",
    "    \n",
    "\n",
    "np.random.seed(0)\n",
    "n = 64\n",
    "x = np.arange(n)\n",
    "kernel = ExpQuadKernel(sigma=1.2, length_scale=5, period=n) + DiagonalKernel(1e-3, n)\n",
    "sample = simulate(x, kernel)\n",
    "plot_sample(sample).legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a kernel with periodic boundary conditions by passing `period=n` to the kernel. The Gaussian process is thus defined on a ring with circumference $n$.\n",
    "\n",
    "To learn the latent rate using stan, we first define the data block of the program, i.e., the information we provide to the inference algorithm. Here, we assume that the marginal scale $\\sigma$, correlation length $\\ell$, and nugget variance $\\epsilon$ are known.\n",
    "\n",
    "```{literalinclude} data.stan\n",
    "   :language: stan\n",
    "```\n",
    "\n",
    "The model is simple: a multivariate normal prior for the log rate $\\eta$ and a Poisson observation model.\n",
    "\n",
    "```{literalinclude} poisson_regression_centered.stan\n",
    "   :language: stan\n",
    "```\n",
    "\n",
    "Let's compile the model, draw posterior samples, and visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from cmdstanpy import CmdStanMCMC\n",
    "from gptools.stan import compile_model\n",
    "from gptools.util import Timer\n",
    "from gptools.util.plotting import plot_band\n",
    "import os\n",
    "\n",
    "\n",
    "def sample_and_plot(stan_file: str, data: dict, return_fit: bool = False, **kwargs) -> CmdStanMCMC:\n",
    "    \"\"\"Draw samples from the posterior and visualize them.\"\"\"\n",
    "    # Set default parameters. We use a small number of samples during testing.\n",
    "    niter = 1 if os.environ.get(\"CI\") else 100\n",
    "    kwargs = {\"iter_warmup\": niter, \"iter_sampling\": niter, \"chains\": 1,\n",
    "              \"refresh\": niter // 10 or None} | kwargs\n",
    "    \n",
    "    # Compile the model and draw posterior samples.\n",
    "    model = compile_model(stan_file=stan_file)\n",
    "    with Timer(f\"sampled using {stan_file}\"):\n",
    "        fit = model.sample(data, **kwargs)\n",
    "        \n",
    "    # Visualize the result.\n",
    "    ax = plot_sample(sample)\n",
    "    plot_band(x, np.exp(fit.stan_variable(\"eta\")), label=\"inferred rate\", ax=ax)\n",
    "    ax.legend()\n",
    "    if return_fit:\n",
    "        return fit\n",
    "    \n",
    "    \n",
    "centered_fit = sample_and_plot(\"poisson_regression_centered.stan\", sample, return_fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stan's Hamiltonian Monte Carlo sampler explores the posterior distribution well and recovers the underlying rate. But it takes its time about it–especially given how small the dataset is. The exploration is slow because adjacenct elements of the log rate are highly correlated under the posterior distribution as shown in the scatter plot below. The mean correlation between adjacent elements is 0.95. The posterior is highly correlated because the Gaussian process prior demands that adjacent points are close: if one moves up, the other must follow to keep the log rate $\\eta$ smooth. If the data are \"strong\", i.e., there is more information in the likelihood than in the prior, the data can decorrelate the posterior: each log rate is primarily informed by the corresponding count rather than the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "etas = centered_fit.stan_variable(\"eta\")\n",
    "idx = 20\n",
    "ax.scatter(etas[:, idx - 1], etas[:, idx]).set_edgecolor(\"w\")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(fr\"Log rate $\\eta_{{{idx}}}$\")\n",
    "ax.set_ylabel(fr\"Log rate $\\eta_{{{idx + 1}}}$\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the data are relatively weak (recall that the variance of the Poisson distribution is equal to its mean such that, for small counts, the likelihood is not particularly informative). We can alleviate this problem by switching to a *non-centered* parameterization such that the parameters are uncorrelated under the prior distribution. In particular, we use a parameter $z\\sim\\text{Normal}\\left(0, 1\\right)$ and transform the white noise such that it is a draw from the Gaussian process prior. This approach is a higher-dimensional analogue of sampling a standard normal distribution and multiplying by the standard deviation $\\sigma$ as opposed to sampling from a normal distribution with standard deviation $\\sigma$ directly. The log rate is\n",
    "$$\n",
    "\\eta = Lz,\n",
    "$$\n",
    "where $L$ is the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) of the covariance matrix $\\Sigma$ of the kernel. The Cholesky decomposition of the covariance matrix is the higher-dimensional analogue of taking the square route of the variance. Because the data are weak, they can only slightly correlate the parameters $z$. The Stan program is shown below.\n",
    "\n",
    "```{literalinclude} poisson_regression_non_centered.stan\n",
    "   :language: stan\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_centered_fit = sample_and_plot(\"poisson_regression_non_centered.stan\", sample, return_fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling from the posterior is substantially faster. While the log rates $\\eta$ remain highly correlated (see left panel below), the posterior for the parameters $z$ the Hamiltonian sampler is exploring are virtually uncorrelated (see right panel below). Takeaway: if the data are strong, use a centered parameterization. If the data are weak, use a non-centered parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "etas = non_centered_fit.stan_variable(\"eta\")\n",
    "zs = non_centered_fit.stan_variable(\"z\")\n",
    "idx = 20\n",
    "ax1.scatter(etas[:, idx - 1], etas[:, idx]).set_edgecolor(\"w\")\n",
    "ax1.set_aspect(\"equal\")\n",
    "ax1.set_xlabel(fr\"Log rate $\\eta_{{{idx}}}$\")\n",
    "ax1.set_ylabel(fr\"Log rate $\\eta_{{{idx + 1}}}$\")\n",
    "\n",
    "ax2.scatter(zs[:, idx - 1], zs[:, idx]).set_edgecolor(\"w\")\n",
    "ax2.set_aspect(\"equal\")\n",
    "ax2.set_xlabel(fr\"Log rate $z_{{{idx}}}$\")\n",
    "ax2.set_ylabel(fr\"Log rate $z_{{{idx + 1}}}$\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest-neighbor Gaussian processes\n",
    "\n",
    "Despite the non-centered parameterization being faster, its performance scales poorly with increasing sample size. Evaluating the Cholesky decomposition scales with $n^3$ such that this approach is only feasible for relatively small datasets. However, intuitively, only local correlations are important for the Gaussian process (at least for the squared exponential kernel we used above). Points separated by a large distance are virtually uncorrelated but these negligible correlations nevertheless slow down our calculations. Fortunately, we can factorize the joint distribution to obtain a product of conditional distributions.\n",
    "$$\n",
    "p\\left(\\eta\\mid \\Sigma\\right)=\\prod_{i=1}^n p\\left(\\eta_{i}\\mid\\eta_{<i},\\Sigma\\right),\n",
    "$$\n",
    "where $\\eta_{<i}$ denotes all parameters $\\left\\{\\eta_1, \\ldots \\eta_{i-1}\\right\\}$ preceeding $\\eta_i$. In a nearest neighbor Gaussian process, we approximate the full conditional by only conditioning on at most the $k$ preceeding parameters, i.e., we neglect all correlations between parameters that are further apart than the $k^\\text{th}$ nearest neighbor. We thus need to decompose $n$ square matrices each having $k + 1$ rows. The computational cost scales as $nk^3$–a substantial saving if $n$ is large compared with $k$.\n",
    "\n",
    "More generally, we can construct a Gaussian process on a directed acyclic graph using the factorization into conditionals. Here, we employ this general formulation and represent the nearest neighbor Gaussian process as a graph. The conditioning structure is illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptools.util.graph import lattice_predecessors, predecessors_to_edge_index\n",
    "\n",
    "k = 5\n",
    "predecessors = lattice_predecessors((n,), k)\n",
    "\n",
    "idx = 20\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(predecessors[idx, -1] - np.arange(3) - 1, np.zeros(3), color=\"silver\", \n",
    "           label=\"ignored nodes\")\n",
    "ax.scatter(idx + np.arange(3) + 1, np.zeros(3), color=\"silver\")\n",
    "ax.scatter(predecessors[idx, 1:], np.zeros(k), label=\"conditioning nodes\")\n",
    "ax.scatter(idx, 0, label=\"target node\")\n",
    "ax.legend()\n",
    "for j in predecessors[idx, :-1]:\n",
    "    ax.annotate(\"\", xy=(j, 0), xytext=(idx, 0), \n",
    "                arrowprops={\"arrowstyle\": \"-|>\", \"connectionstyle\": \"arc3,rad=.5\"})\n",
    "ax.set_ylim(-.25, .25)\n",
    "ax.set_axis_off()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `lattice_predecessors` constructs a nearest neighbor matrix `predecessors` with shape `(n, k + 1)` such that the first `k` elements of the $i^\\text{th}$ row are the predecessors of node $i$ and the last element is the node itself. The corresponding Stan program is shown below. The distribution `graph_gp` encodes the Gaussian process.\n",
    "\n",
    "```{literalinclude} poisson_regression_graph_centered.stan\n",
    "   :language: stan\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = predecessors_to_edge_index(predecessors)\n",
    "sample |= {\"edge_index\": edges, \"num_edges\": edges.shape[1]}\n",
    "sample_and_plot(\"poisson_regression_graph_centered.stan\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centered graph Gaussian process is indeed faster than the standard implementation. However, it suffers from the same challenges: the posterior for $\\eta$ is highly correlated. Fortunately, we can also construct a non-centered parameterization.\n",
    "\n",
    "```{literalinclude} poisson_regression_graph_non_centered.stan\n",
    "   :language: stan\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = predecessors_to_edge_index(predecessors)\n",
    "sample |= {\"edge_index\": edges, \"num_edges\": edges.shape[1]}\n",
    "sample_and_plot(\"poisson_regression_graph_non_centered.stan\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-centered parameterization of the graph Gaussian process is even faster, and we have reduced the runtime by more than an order of magnitude for this small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian process using fast Fourier transforms\n",
    "\n",
    "If, as in this example, observations are regularly spaced, we can evaluate the likelihood using the fast Fourier transform. Similarly, we can construct a non-centered parameterization by transforming Fourier coefficients with {stan:func}`gp_transform_irfft`.\n",
    "\n",
    "```{literalinclude} poisson_regression_fourier_centered.stan\n",
    "   :language: stan\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_plot(\"poisson_regression_fourier_centered.stan\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_plot(\"poisson_regression_fourier_non_centered.stan\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the substantial performance improvements, we can readily increase the sample size as illustrated below for a dataset with more than a thousand observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1024)\n",
    "kernel = ExpQuadKernel(sigma=1.2, length_scale=15, period=x.size) + DiagonalKernel(1e-3, x.size)\n",
    "sample = simulate(x, kernel)\n",
    "plot_sample(sample).legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_plot(\"poisson_regression_fourier_non_centered.stan\", sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce292631760a50db98487f107c57a4e83e7303c2c65ea0b6fb35e4138a49650e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
