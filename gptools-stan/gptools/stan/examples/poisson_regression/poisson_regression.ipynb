{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmdstanpy\n",
    "from gptools.stan import compile_model\n",
    "from gptools.stan.examples import sample_kwargs_from_env\n",
    "from gptools.stan.examples.poisson_regression import simulate, plot_realization_1d\n",
    "from gptools.util import Timer\n",
    "from gptools.util.graph import lattice_predecessors, predecessors_to_edge_index\n",
    "from gptools.util.kernels import ExpQuadKernel\n",
    "from gptools.util.plotting import plot_band\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a simple Poisson regression model for count data $y$ with a latent log-rate $\\eta$ subject to a Gaussian process prior. The model is specified by\n",
    "$$\\begin{align}\n",
    "\\eta &\\sim \\text{MultivariateNormal}\\left(\\mu, K\\left(x,\\alpha,\\rho,\\epsilon\\right)\\right)\\\\\n",
    "y & \\sim \\text{Poisson}\\left(\\exp\\eta\\right),\n",
    "\\end{align}$$\n",
    "where $x$ are the observation coordinates. We use a radial basis function kernel $K$ such that\n",
    "$$\n",
    "\\text{cov}\\left(\\eta_i,\\eta_j\\right) = \\alpha^2 \\exp\\left(-\\frac{\\left\\vert x_i-x_j\\right\\vert^2}{2\\rho^2}\\right) + \\epsilon\\delta_{ij},\n",
    "$$\n",
    "where $\\alpha$ is the covariance scale, $\\rho$ is the correlation length, and $\\epsilon$ is a diagonal noise term to ensure the covariance is positive-definite. A realization of the process is shown below. For simplicity, we assume that the kernel parameters are known in this example and use periodic boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters of the model and simulate forwards.\n",
    "np.random.seed(1)\n",
    "n = 100\n",
    "kernel = ExpQuadKernel(1.2, 5, 1e-3, n)\n",
    "mu = 1\n",
    "x = np.arange(n)\n",
    "realization = simulate(x, mu, kernel)\n",
    "plot_realization_1d(realization).legend()\n",
    "\n",
    "predecessors = lattice_predecessors(x.shape, 5)\n",
    "edge_index = predecessors_to_edge_index(predecessors)\n",
    "\n",
    "realization.update({\n",
    "    \"edge_index\": edge_index,\n",
    "    \"num_edges\": edge_index.shape[1],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Gaussian process implementation\n",
    "\n",
    "We first fit the model using a centered parameterization, i.e., the parameters of the sampler directly correspond to the latent log-rate $\\eta$. This parameterization is intuitive, but it can be inefficient when the data are not strongly informative. In this example, the information provided by the counts $y$ is relatively low because the mean rate is small. Consequently, $\\eta$ is primarily constrained by the Gaussian process prior such that adjacent points are highly correlated (the data cannot decorrelated them). Posterior samples are thus highly correlated which slows down the sampler. But let's fit this model anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_plot(stan_file: str, **kwargs) -> cmdstanpy.CmdStanMCMC:\n",
    "    model = compile_model(stan_file=stan_file)\n",
    "    with Timer(f\"sampled using {stan_file}\"):\n",
    "        fit = model.sample(realization, **(sample_kwargs_from_env() | kwargs))\n",
    "        \n",
    "    ax = plot_realization_1d(realization)\n",
    "    plot_band(x, np.exp(fit.stan_variable(\"eta\")), label=\"inferred rate\")\n",
    "    ax.legend()\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centered_fit = sample_and_plot(\"poisson_regression_centered.stan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model successfully infers the latent rate. However, in light of the weak data, we consider a non-centered parameterization of the model. In other words, we use a parameter $z\\sim\\text{Normal}\\left(0, 1\\right)$ and transform the white noise to a sample from the Gaussian process. This operation is analogous to sampling from a univariate Gaussian with mean $\\mu$ and scale $\\sigma$ by sampling from a standard Gaussian, multiplying by the scale $\\sigma$, and adding the mean $\\mu$. The two approaches are equivalent but the non-centered parameterization facilitates faster sampling for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_centered_fit = sample_and_plot(\"poisson_regression_non_centered.stan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling succeeds as before, but the inference is faster because the white noise $z$ is uncorrelated under the prior and the weak data $y$ only induce minimal correlation. Let's consider the tree depth used by the sampler to explore the posterior (larger tree depths correspond to less efficient sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for key, fit in [(\"centered\", centered_fit), (\"non_centered\", non_centered_fit)]:\n",
    "    variables = fit.method_variables()\n",
    "    treedepths = np.bincount(variables[\"treedepth__\"].astype(int).ravel())\n",
    "    ax.plot(treedepths, marker='o', label=key)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Tree depth\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of samples from the centered parameterization have tree depth greater than six. But all samples from the non-centered parametriazation have tree depth of six or less, explaining the difference in sampling efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph likelihood\n",
    "\n",
    "Considering only nearest neighbors in the precision matrix reduces the computational burden because we only need to invert many small matrices rather than one big one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_plot(\"poisson_regression_graph_centered.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_plot(\"poisson_regression_graph_non_centered.stan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier likelihood\n",
    "\n",
    "Employing the Fourier transform, we can evaluate the likelihood exactly without needing to invert the matrix if the grid is regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_plot(\"poisson_regression_fourier_centered.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_plot(\"poisson_regression_fourier_non_centered.stan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce292631760a50db98487f107c57a4e83e7303c2c65ea0b6fb35e4138a49650e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
